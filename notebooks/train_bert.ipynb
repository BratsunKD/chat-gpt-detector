{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-15T20:07:28.091497Z",
     "iopub.status.busy": "2024-11-15T20:07:28.091075Z",
     "iopub.status.idle": "2024-11-15T20:07:28.718329Z",
     "shell.execute_reply": "2024-11-15T20:07:28.717249Z",
     "shell.execute_reply.started": "2024-11-15T20:07:28.091444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gptnew/gpt.json\n",
      "/kaggle/input/gpthuman/otvet.mail.ru/otvet.mail.ru/gpt.json\n",
      "/kaggle/input/gpthuman/otvet.mail.ru/otvet.mail.ru/people.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:07:39.238229Z",
     "iopub.status.busy": "2024-11-15T20:07:39.237301Z",
     "iopub.status.idle": "2024-11-15T20:08:08.351612Z",
     "shell.execute_reply": "2024-11-15T20:08:08.350705Z",
     "shell.execute_reply.started": "2024-11-15T20:07:39.238186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.42.3 in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Collecting transformers>=4.42.3\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.1/44.1 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.42.3) (4.66.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.42.3) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.0/10.0 MB\u001B[0m \u001B[31m75.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m:01\u001B[0m\n",
      "\u001B[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m122.4/122.4 MB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m333.2/333.2 kB\u001B[0m \u001B[31m19.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m320.7/320.7 kB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: bitsandbytes, accelerate, transformers, peft\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.34.2\n",
      "    Uninstalling accelerate-0.34.2:\n",
      "      Successfully uninstalled accelerate-0.34.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed accelerate-1.1.1 bitsandbytes-0.44.1 peft-0.13.2 transformers-4.46.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:08:25.739555Z",
     "iopub.status.busy": "2024-11-15T20:08:25.738747Z",
     "iopub.status.idle": "2024-11-15T20:08:44.910426Z",
     "shell.execute_reply": "2024-11-15T20:08:44.909661Z",
     "shell.execute_reply.started": "2024-11-15T20:08:25.739516Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, XLMRobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, TaskType\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Инициализация Accelerate\n",
    "accelerator = Accelerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:08:49.796364Z",
     "iopub.status.busy": "2024-11-15T20:08:49.795599Z",
     "iopub.status.idle": "2024-11-15T20:08:50.033590Z",
     "shell.execute_reply": "2024-11-15T20:08:50.032643Z",
     "shell.execute_reply.started": "2024-11-15T20:08:49.796324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                               text  target\n",
      "0  1079405  Есть французы!!! Perdu - потеряный , от perdre...       0\n",
      "1  1079517  запомни: в последней фразе, которую ты скажешь...       0\n",
      "2  1079574  Заговорит. Наверняка хоть что-то и как-то он п...       0\n",
      "3  1079678  сосуд с мочой предвещает ухудшение здоровья и ...       0\n",
      "4  1079681  METALLICA рулит!!! Что бы там всякие папазоглы...       0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Функция для загрузки и предобработки данных с человеческими ответами\n",
    "def load_human_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        \n",
    "        answer = f\"{item['answer']['text']}\"\n",
    "        \n",
    "        # Склеиваем и чистим текст\n",
    "        combined_text = f\"{answer}\"\n",
    "        combined_text = re.sub(r\"[\\\"']\", \"\", combined_text)  # Удаление кавычек\n",
    "        combined_text = re.sub(r\"\\s+\", \" \", combined_text).strip()  # Удаление лишних пробелов\n",
    "        cleaned_data.append({\"id\": item['id'], \"text\": combined_text, \"target\": 0})\n",
    "    return pd.DataFrame(cleaned_data)\n",
    "\n",
    "# Функция для загрузки и предобработки данных с машинными ответами\n",
    "def load_machine_data(file_path, human_data_df):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)[0]  # Извлекаем первый элемент списка, который является словарем\n",
    "    cleaned_data = []\n",
    "    for id, text in data.items():\n",
    "        if int(id) in human_data_df['id'].values:  # Проверка на соответствие ID\n",
    "\n",
    "            \n",
    "            # Склеиваем и чистим текст\n",
    "            combined_text = f\"{text}\"\n",
    "            combined_text = re.sub(r\"[\\\"']\", \"\", combined_text)  # Удаление кавычек\n",
    "            combined_text = re.sub(r\"\\s+\", \" \", combined_text).strip()  # Удаление лишних пробелов\n",
    "            cleaned_data.append({\"id\": int(id), \"text\": combined_text, \"target\": 1})\n",
    "    return pd.DataFrame(cleaned_data)\n",
    "\n",
    "# Путь к файлам\n",
    "human_data_path = '/kaggle/input/gpthuman/otvet.mail.ru/otvet.mail.ru/people.json'\n",
    "machine_data_path = '/kaggle/input/gptnew/gpt.json'\n",
    "\n",
    "# Загрузка и обработка данных\n",
    "human_df = load_human_data(human_data_path)\n",
    "machine_df = load_machine_data(machine_data_path, human_df)\n",
    "\n",
    "# Объединение данных\n",
    "merged_df = pd.concat([human_df, machine_df], ignore_index=True)\n",
    "\n",
    "# Проверка результата\n",
    "print(merged_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-10T21:56:30.770802Z",
     "iopub.status.busy": "2024-11-10T21:56:30.770315Z",
     "iopub.status.idle": "2024-11-10T21:56:30.779589Z",
     "shell.execute_reply": "2024-11-10T21:56:30.778216Z",
     "shell.execute_reply.started": "2024-11-10T21:56:30.770754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-10T21:46:15.579974Z",
     "iopub.status.idle": "2024-11-10T21:46:15.580473Z",
     "shell.execute_reply": "2024-11-10T21:46:15.580250Z",
     "shell.execute_reply.started": "2024-11-10T21:46:15.580228Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:09:00.275798Z",
     "iopub.status.busy": "2024-11-15T20:09:00.275155Z",
     "iopub.status.idle": "2024-11-15T20:21:53.924546Z",
     "shell.execute_reply": "2024-11-15T20:21:53.923143Z",
     "shell.execute_reply.started": "2024-11-15T20:09:00.275759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e49a856b4ad416ab9b6f401feefdbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07de934b7464d269ae3b9c67433fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e813ef0b566c47a08d30f2352890c654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de7a9898b02432fa4868df6b19881d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ce0ead039e40f296875164751a1579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sagteam/xlm-roberta-large-sag and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_30/2319582127.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='645' max='7650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 645/7650 12:32 < 2:16:38, 0.85 it/s, Epoch 0.84/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.284496</td>\n",
       "      <td>0.935948</td>\n",
       "      <td>0.938053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.241885</td>\n",
       "      <td>0.962092</td>\n",
       "      <td>0.961992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.194270</td>\n",
       "      <td>0.971242</td>\n",
       "      <td>0.970667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.471913</td>\n",
       "      <td>0.945098</td>\n",
       "      <td>0.946429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.212691</td>\n",
       "      <td>0.966013</td>\n",
       "      <td>0.966057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.135407</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.975676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.134356</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.117958</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.102335</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.984127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.102497</td>\n",
       "      <td>0.986928</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.208243</td>\n",
       "      <td>0.969935</td>\n",
       "      <td>0.969935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142934</td>\n",
       "      <td>0.981699</td>\n",
       "      <td>0.981432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>0.640600</td>\n",
       "      <td>0.089503</td>\n",
       "      <td>0.986928</td>\n",
       "      <td>0.986702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a93e9505baf4feb8560ed6faff3d5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 90\u001B[0m\n\u001B[1;32m     85\u001B[0m trainer\u001B[38;5;241m.\u001B[39mmodel, trainer\u001B[38;5;241m.\u001B[39mtrain_dataloader, trainer\u001B[38;5;241m.\u001B[39meval_dataloader \u001B[38;5;241m=\u001B[39m accelerator\u001B[38;5;241m.\u001B[39mprepare(\n\u001B[1;32m     86\u001B[0m     trainer\u001B[38;5;241m.\u001B[39mmodel, trainer\u001B[38;5;241m.\u001B[39mget_train_dataloader(), trainer\u001B[38;5;241m.\u001B[39mget_eval_dataloader()\n\u001B[1;32m     87\u001B[0m )\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Обучение модели\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;66;03m# Финальная оценка на валидационном наборе\u001B[39;00m\n\u001B[1;32m     93\u001B[0m final_eval_results \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mevaluate(eval_dataset\u001B[38;5;241m=\u001B[39mdev_dataset)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2121\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2124\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2486\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   2481\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2484\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2485\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m-> 2486\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misinf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss_step\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2487\u001B[0m ):\n\u001B[1;32m   2488\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2489\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n\u001B[1;32m   2490\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "combined_data = merged_df[['text', 'target']].to_dict(orient='records')\n",
    "\n",
    "class BinaryClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.texts = [item['text'] for item in data]\n",
    "        self.labels = [item['target'] for item in data]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        tokens = {key: val.squeeze(0) for key, val in tokens.items()}  # Убираем первую размерность\n",
    "        tokens['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagteam/xlm-roberta-large-sag\")\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('sagteam/xlm-roberta-large-sag', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    layers_to_transform=[i for i in range(24) if i >= 15]\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "dataset = BinaryClassificationDataset(combined_data, tokenizer, max_length=512)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, dev_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    optim=\"adamw_8bit\",\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(p.label_ids, preds)\n",
    "    f1 = f1_score(p.label_ids, preds)\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# Подготовка модели и данных для использования с Accelerator\n",
    "trainer.model, trainer.train_dataloader, trainer.eval_dataloader = accelerator.prepare(\n",
    "    trainer.model, trainer.get_train_dataloader(), trainer.get_eval_dataloader()\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# Финальная оценка на валидационном наборе\n",
    "final_eval_results = trainer.evaluate(eval_dataset=dev_dataset)\n",
    "print(f\"Final evaluation results on dev dataset: {final_eval_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:22:04.202473Z",
     "iopub.status.busy": "2024-11-15T20:22:04.202099Z",
     "iopub.status.idle": "2024-11-15T20:22:46.067560Z",
     "shell.execute_reply": "2024-11-15T20:22:46.066493Z",
     "shell.execute_reply.started": "2024-11-15T20:22:04.202439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation results on dev dataset: {'eval_loss': 0.08950305730104446, 'eval_accuracy': 0.9869281045751634, 'eval_f1': 0.9867021276595745}\n"
     ]
    }
   ],
   "source": [
    "final_eval_results = trainer.evaluate(eval_dataset=dev_dataset)\n",
    "print(f\"Final evaluation results on dev dataset: {final_eval_results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/kaggle/working/\"\n",
    "\n",
    "# Сохранение только LoRA весов\n",
    "model.save_pretrained(lora_save_path)\n",
    "tokenizer.save_pretrained(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T19:32:04.804670Z",
     "iopub.status.busy": "2024-11-15T19:32:04.803790Z",
     "iopub.status.idle": "2024-11-15T19:32:04.901645Z",
     "shell.execute_reply": "2024-11-15T19:32:04.900486Z",
     "shell.execute_reply.started": "2024-11-15T19:32:04.804626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot display a directory using FileLink. Use FileLinks to display 'output/checkpoint-400'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcd\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/kaggle/working\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FileLink\n\u001B[0;32m----> 3\u001B[0m \u001B[43mFileLink\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput/checkpoint-400\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/IPython/lib/display.py:407\u001B[0m, in \u001B[0;36mFileLink.__init__\u001B[0;34m(self, path, url_prefix, result_html_prefix, result_html_suffix)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;124;03m----------\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    404\u001B[0m \u001B[38;5;124;03m    text to append at the end of link [default: '<br>']\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m isdir(path):\n\u001B[0;32m--> 407\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot display a directory using FileLink. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    408\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse FileLinks to display \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m path)\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m fsdecode(path)\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39murl_prefix \u001B[38;5;241m=\u001B[39m url_prefix\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot display a directory using FileLink. Use FileLinks to display 'output/checkpoint-400'."
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working\n",
    "from IPython.display import FileLink\n",
    "FileLink('output/checkpoint-400')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:27:42.889106Z",
     "iopub.status.busy": "2024-11-15T20:27:42.888458Z",
     "iopub.status.idle": "2024-11-15T20:27:42.897078Z",
     "shell.execute_reply": "2024-11-15T20:27:42.896042Z",
     "shell.execute_reply.started": "2024-11-15T20:27:42.889066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Функция для инициализации модели и загрузки LoRA весов\n",
    "def initialize_model_with_lora(base_model_name, lora_weights_path):\n",
    "    # Загружаем базовую модель и токенизатор\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
    "    \n",
    "    # Загружаем LoRA-адаптеры\n",
    "    model = PeftModel.from_pretrained(model, lora_weights_path)\n",
    "    model.eval()  # Переводим модель в режим оценки (inference)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Функция для обработки текста\n",
    "def process_text(model, tokenizer, text, max_length=512):\n",
    "    # Токенизация входного текста\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Прогон через модель\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    # Получение предсказаний\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).item()\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"sagteam/xlm-roberta-large-sag\"\n",
    "lora_weights_path = \"/kaggle/working/output/checkpoint-500\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:27:58.398107Z",
     "iopub.status.busy": "2024-11-15T20:27:58.397261Z",
     "iopub.status.idle": "2024-11-15T20:28:02.601669Z",
     "shell.execute_reply": "2024-11-15T20:28:02.600713Z",
     "shell.execute_reply.started": "2024-11-15T20:27:58.398069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sagteam/xlm-roberta-large-sag and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = initialize_model_with_lora(base_model_name, lora_weights_path)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n",
    "prediction = process_text(model, tokenizer, user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T20:30:05.369535Z",
     "iopub.status.busy": "2024-11-15T20:30:05.368792Z",
     "iopub.status.idle": "2024-11-15T20:30:06.928685Z",
     "shell.execute_reply": "2024-11-15T20:30:06.927658Z",
     "shell.execute_reply.started": "2024-11-15T20:30:05.369493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Классификация: Сгенерирован\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Для того чтобы использовать код, который вы привели, необходимо создать объект combined_data из вашего DataFrame df, где будут использованы столбцы text и target. Далее мы можем преобразовать этот DataFrame в нужный формат — список словарей, где каждый словарь содержит text и target. Также потребуется импортировать необходимые модули, если они еще не импортированы.\"\n",
    "prediction = process_text(model, tokenizer, user_input)\n",
    "\n",
    "# Интерпретация результата\n",
    "if prediction == 1:\n",
    "    print(\"Классификация: Написан человеком\")\n",
    "else:\n",
    "    print(\"Классификация: Сгенерирован\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6052926,
     "sourceId": 9862159,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6055592,
     "sourceId": 9865696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
